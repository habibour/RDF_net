{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f31d44aa",
   "metadata": {},
   "source": [
    "# üöÄ RDFNet Training & Evaluation\n",
    "\n",
    "**Object Detection in Foggy Scenes**\n",
    "\n",
    "This notebook trains RDFNet from scratch on your VOC-FOG dataset and evaluates on RTTS.\n",
    "\n",
    "---\n",
    "\n",
    "**Your Dataset Paths:**\n",
    "- Training: `/content/drive/MyDrive/dataset/training/VOC2007 2/`\n",
    "- Testing (RTTS): `/content/drive/MyDrive/dataset/RTTS/VOC2007/`\n",
    "- Checkpoints: `/content/drive/MyDrive/RDFNet_training_checkpoints/`\n",
    "\n",
    "---\n",
    "\n",
    "| Step | Action |\n",
    "|------|--------|\n",
    "| **Fresh Start** | Run cells 1-7 |\n",
    "| **Resume** | Run cells 1-4, then 5 |\n",
    "| **Evaluate** | Run cells 1-4, then 8-9 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622b28db",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Mount Drive & Clone Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c56d55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Clone fresh repo from GitHub\n",
    "!rm -rf RDF_net\n",
    "!git clone https://github.com/habibour/RDF_net.git\n",
    "%cd RDF_net\n",
    "\n",
    "# Install dependencies\n",
    "!pip install thop -q\n",
    "\n",
    "print(\"\\n‚úÖ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed27c033",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Verify Dataset Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb9d326",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Your paths\n",
    "TRAIN_FOG = '/content/drive/MyDrive/dataset/training/VOC2007 2/FOG'\n",
    "TRAIN_ANN = '/content/drive/MyDrive/dataset/training/VOC2007 2/Annotations'\n",
    "TEST_IMG = '/content/drive/MyDrive/dataset/RTTS/VOC2007/JPEGImages'\n",
    "TEST_ANN = '/content/drive/MyDrive/dataset/RTTS/VOC2007/Annotations'\n",
    "CHECKPOINT_DIR = '/content/drive/MyDrive/RDFNet_training_checkpoints'\n",
    "\n",
    "print(\"üìÅ Training Dataset:\")\n",
    "if os.path.exists(TRAIN_FOG):\n",
    "    print(f\"  ‚úÖ FOG images: {len(os.listdir(TRAIN_FOG))}\")\n",
    "else:\n",
    "    print(f\"  ‚ùå FOG path not found: {TRAIN_FOG}\")\n",
    "\n",
    "if os.path.exists(TRAIN_ANN):\n",
    "    print(f\"  ‚úÖ Annotations: {len(os.listdir(TRAIN_ANN))}\")\n",
    "else:\n",
    "    print(f\"  ‚ùå Annotations path not found: {TRAIN_ANN}\")\n",
    "\n",
    "print(\"\\nüìÅ Testing Dataset (RTTS):\")\n",
    "if os.path.exists(TEST_IMG):\n",
    "    print(f\"  ‚úÖ Images: {len(os.listdir(TEST_IMG))}\")\n",
    "else:\n",
    "    print(f\"  ‚ùå Images path not found: {TEST_IMG}\")\n",
    "\n",
    "if os.path.exists(TEST_ANN):\n",
    "    print(f\"  ‚úÖ Annotations: {len(os.listdir(TEST_ANN))}\")\n",
    "else:\n",
    "    print(f\"  ‚ùå Annotations path not found: {TEST_ANN}\")\n",
    "\n",
    "print(\"\\nüìÅ Checkpoints:\")\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "existing = [f for f in os.listdir(CHECKPOINT_DIR) if f.endswith('.pth')]\n",
    "if existing:\n",
    "    print(f\"  üì¶ Found {len(existing)} existing checkpoints\")\n",
    "    for f in sorted(existing)[-3:]:\n",
    "        print(f\"      - {f}\")\n",
    "else:\n",
    "    print(\"  üì≠ No existing checkpoints (fresh training)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e583d4",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Generate Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f75c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import random\n",
    "\n",
    "# Paths\n",
    "TRAIN_FOG = '/content/drive/MyDrive/dataset/training/VOC2007 2/FOG'\n",
    "TRAIN_ANN = '/content/drive/MyDrive/dataset/training/VOC2007 2/Annotations'\n",
    "TEST_IMG = '/content/drive/MyDrive/dataset/RTTS/VOC2007/JPEGImages'\n",
    "TEST_ANN = '/content/drive/MyDrive/dataset/RTTS/VOC2007/Annotations'\n",
    "\n",
    "# Classes\n",
    "VOC_CLASSES = [\"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\", \"bus\", \"car\", \"cat\", \n",
    "               \"chair\", \"cow\", \"diningtable\", \"dog\", \"horse\", \"motorbike\", \"person\", \n",
    "               \"pottedplant\", \"sheep\", \"sofa\", \"train\", \"tvmonitor\"]\n",
    "RTTS_CLASSES = ['bicycle', 'bus', 'car', 'motorbike', 'person']\n",
    "\n",
    "def convert_annotation(xml_path, classes):\n",
    "    try:\n",
    "        tree = ET.parse(xml_path)\n",
    "        root = tree.getroot()\n",
    "        boxes = []\n",
    "        for obj in root.iter('object'):\n",
    "            difficult = obj.find('difficult')\n",
    "            if difficult is not None and difficult.text == '1':\n",
    "                continue\n",
    "            name = obj.find('name').text\n",
    "            if name not in classes:\n",
    "                continue\n",
    "            bbox = obj.find('bndbox')\n",
    "            b = (int(float(bbox.find('xmin').text)), int(float(bbox.find('ymin').text)),\n",
    "                 int(float(bbox.find('xmax').text)), int(float(bbox.find('ymax').text)))\n",
    "            boxes.append(f\"{b[0]},{b[1]},{b[2]},{b[3]},{classes.index(name)}\")\n",
    "        return boxes\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "# === Generate Training Annotations ===\n",
    "print(\"=\"*60)\n",
    "print(\"Generating Training Annotations\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "fog_files = set(os.listdir(TRAIN_FOG))\n",
    "xml_files = [f[:-4] for f in os.listdir(TRAIN_ANN) if f.endswith('.xml')]\n",
    "valid_ids = [x for x in xml_files if f\"{x}.jpg\" in fog_files]\n",
    "\n",
    "print(f\"FOG images: {len(fog_files)}\")\n",
    "print(f\"XML annotations: {len(xml_files)}\")\n",
    "print(f\"Valid pairs: {len(valid_ids)}\")\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(valid_ids)\n",
    "train_split = int(len(valid_ids) * 0.9)\n",
    "train_ids = valid_ids[:train_split]\n",
    "val_ids = valid_ids[train_split:]\n",
    "\n",
    "# Write train\n",
    "train_count = 0\n",
    "with open('2007_train.txt', 'w') as f:\n",
    "    for img_id in train_ids:\n",
    "        img_path = f\"{TRAIN_FOG}/{img_id}.jpg\"\n",
    "        boxes = convert_annotation(f\"{TRAIN_ANN}/{img_id}.xml\", VOC_CLASSES)\n",
    "        if boxes:\n",
    "            f.write(f\"{img_path} {' '.join(boxes)}\\n\")\n",
    "            train_count += 1\n",
    "\n",
    "# Write val\n",
    "val_count = 0\n",
    "with open('2007_val.txt', 'w') as f:\n",
    "    for img_id in val_ids:\n",
    "        img_path = f\"{TRAIN_FOG}/{img_id}.jpg\"\n",
    "        boxes = convert_annotation(f\"{TRAIN_ANN}/{img_id}.xml\", VOC_CLASSES)\n",
    "        if boxes:\n",
    "            f.write(f\"{img_path} {' '.join(boxes)}\\n\")\n",
    "            val_count += 1\n",
    "\n",
    "print(f\"\\n‚úÖ Train: {train_count} images\")\n",
    "print(f\"‚úÖ Val: {val_count} images\")\n",
    "\n",
    "# Create VOC classes file\n",
    "os.makedirs('model_data', exist_ok=True)\n",
    "with open('model_data/voc_classes.txt', 'w') as f:\n",
    "    f.write('\\n'.join(VOC_CLASSES))\n",
    "print(\"‚úÖ Created voc_classes.txt\")\n",
    "\n",
    "# === Generate RTTS Test Annotations ===\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Generating RTTS Test Annotations\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_count = 0\n",
    "with open('rtts_test.txt', 'w') as f:\n",
    "    for xml_file in os.listdir(TEST_ANN):\n",
    "        if not xml_file.endswith('.xml'):\n",
    "            continue\n",
    "        img_id = xml_file[:-4]\n",
    "        img_file = None\n",
    "        for ext in ['.jpg', '.png', '.jpeg', '.JPG']:\n",
    "            if os.path.exists(os.path.join(TEST_IMG, img_id + ext)):\n",
    "                img_file = img_id + ext\n",
    "                break\n",
    "        if img_file:\n",
    "            boxes = convert_annotation(os.path.join(TEST_ANN, xml_file), RTTS_CLASSES)\n",
    "            if boxes:\n",
    "                f.write(f\"{TEST_IMG}/{img_file} {' '.join(boxes)}\\n\")\n",
    "                test_count += 1\n",
    "\n",
    "print(f\"‚úÖ RTTS test: {test_count} images\")\n",
    "\n",
    "# Create RTTS classes file\n",
    "with open('model_data/rtts_classes.txt', 'w') as f:\n",
    "    f.write('\\n'.join(RTTS_CLASSES))\n",
    "print(\"‚úÖ Created rtts_classes.txt\")\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e26cfc2",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Setup Config for Fresh Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5667ba42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config for FRESH training (using only pretrained backbone)\n",
    "config_content = '''\n",
    "import os\n",
    "\n",
    "# Dataset Paths\n",
    "TRAIN_FOG_PATH = '/content/drive/MyDrive/dataset/training/VOC2007 2/FOG'\n",
    "TRAIN_ANN_PATH = '/content/drive/MyDrive/dataset/training/VOC2007 2/Annotations'\n",
    "TEST_IMG_PATH = '/content/drive/MyDrive/dataset/RTTS/VOC2007/JPEGImages'\n",
    "TEST_ANN_PATH = '/content/drive/MyDrive/dataset/RTTS/VOC2007/Annotations'\n",
    "CHECKPOINT_DIR = '/content/drive/MyDrive/RDFNet_training_checkpoints'\n",
    "\n",
    "VOC_CLASSES = [\"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\", \"bus\", \"car\", \"cat\", \n",
    "               \"chair\", \"cow\", \"diningtable\", \"dog\", \"horse\", \"motorbike\", \"person\", \n",
    "               \"pottedplant\", \"sheep\", \"sofa\", \"train\", \"tvmonitor\"]\n",
    "RTTS_CLASSES = ['bicycle', 'bus', 'car', 'motorbike', 'person']\n",
    "\n",
    "# Model (fresh training - only backbone weights)\n",
    "Cuda = True\n",
    "seed = 114514\n",
    "distributed = False\n",
    "sync_bn = False\n",
    "fp16 = True\n",
    "model_path = 'model_data/yolov7_tiny_weights.pth'\n",
    "classes_path = 'model_data/voc_classes.txt'\n",
    "anchors_path = 'model_data/yolo_anchors.txt'\n",
    "anchors_mask = [[6, 7, 8], [3, 4, 5], [0, 1, 2]]\n",
    "input_shape = [640, 640]\n",
    "pretrained = False\n",
    "\n",
    "# Training\n",
    "Init_Epoch = 0\n",
    "Freeze_Epoch = 100\n",
    "Freeze_batch_size = 16\n",
    "UnFreeze_Epoch = 300\n",
    "Unfreeze_batch_size = 8\n",
    "Freeze_Train = True\n",
    "\n",
    "# Optimizer\n",
    "Init_lr = 1e-2\n",
    "Min_lr = Init_lr * 0.01\n",
    "optimizer_type = \"sgd\"\n",
    "momentum = 0.937\n",
    "weight_decay = 5e-4\n",
    "lr_decay_type = \"cos\"\n",
    "\n",
    "# Save\n",
    "save_period = 10\n",
    "save_dir = 'logs'\n",
    "eval_flag = True\n",
    "eval_period = 10\n",
    "num_workers = 4\n",
    "\n",
    "# Annotations\n",
    "train_annotation_path = '2007_train.txt'\n",
    "val_annotation_path = '2007_val.txt'\n",
    "'''\n",
    "\n",
    "with open('colab_config.py', 'w') as f:\n",
    "    f.write(config_content)\n",
    "\n",
    "print(\"‚úÖ Config set for FRESH training\")\n",
    "print(\"   Using: yolov7_tiny_weights.pth (backbone only)\")\n",
    "print(\"   NOT using any saved checkpoints\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af3d294",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Resume Training (‚ö†Ô∏è ONLY RUN IF RESUMING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3104455f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# ‚ö†Ô∏è RUN THIS CELL ONLY IF YOU WANT TO RESUME\n",
    "# =============================================\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "CHECKPOINT_DIR = '/content/drive/MyDrive/RDFNet_training_checkpoints'\n",
    "os.makedirs('logs', exist_ok=True)\n",
    "\n",
    "# Find latest checkpoint\n",
    "checkpoints = [f for f in os.listdir(CHECKPOINT_DIR) if f.startswith('ep') and f.endswith('.pth')]\n",
    "\n",
    "if checkpoints:\n",
    "    checkpoints.sort(key=lambda x: int(x.split('-')[0][2:]))\n",
    "    latest = checkpoints[-1]\n",
    "    latest_epoch = int(latest.split('-')[0][2:])\n",
    "    \n",
    "    # Copy to local logs\n",
    "    shutil.copy2(f'{CHECKPOINT_DIR}/{latest}', f'logs/{latest}')\n",
    "    \n",
    "    print(f\"üìå Resuming from: {latest}\")\n",
    "    print(f\"üìå Starting at epoch: {latest_epoch}\")\n",
    "    \n",
    "    # Update config for resume\n",
    "    resume_config = f'''\n",
    "import os\n",
    "\n",
    "TRAIN_FOG_PATH = '/content/drive/MyDrive/dataset/training/VOC2007 2/FOG'\n",
    "TRAIN_ANN_PATH = '/content/drive/MyDrive/dataset/training/VOC2007 2/Annotations'\n",
    "TEST_IMG_PATH = '/content/drive/MyDrive/dataset/RTTS/VOC2007/JPEGImages'\n",
    "TEST_ANN_PATH = '/content/drive/MyDrive/dataset/RTTS/VOC2007/Annotations'\n",
    "CHECKPOINT_DIR = '/content/drive/MyDrive/RDFNet_training_checkpoints'\n",
    "\n",
    "VOC_CLASSES = [\"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\", \"bus\", \"car\", \"cat\", \n",
    "               \"chair\", \"cow\", \"diningtable\", \"dog\", \"horse\", \"motorbike\", \"person\", \n",
    "               \"pottedplant\", \"sheep\", \"sofa\", \"train\", \"tvmonitor\"]\n",
    "RTTS_CLASSES = ['bicycle', 'bus', 'car', 'motorbike', 'person']\n",
    "\n",
    "Cuda = True\n",
    "seed = 114514\n",
    "distributed = False\n",
    "sync_bn = False\n",
    "fp16 = True\n",
    "model_path = 'logs/{latest}'  # Resume from checkpoint\n",
    "classes_path = 'model_data/voc_classes.txt'\n",
    "anchors_path = 'model_data/yolo_anchors.txt'\n",
    "anchors_mask = [[6, 7, 8], [3, 4, 5], [0, 1, 2]]\n",
    "input_shape = [640, 640]\n",
    "pretrained = False\n",
    "\n",
    "Init_Epoch = {latest_epoch}\n",
    "Freeze_Epoch = 100\n",
    "Freeze_batch_size = 16\n",
    "UnFreeze_Epoch = 300\n",
    "Unfreeze_batch_size = 8\n",
    "Freeze_Train = {'True' if latest_epoch < 100 else 'False'}\n",
    "\n",
    "Init_lr = 1e-2\n",
    "Min_lr = Init_lr * 0.01\n",
    "optimizer_type = \"sgd\"\n",
    "momentum = 0.937\n",
    "weight_decay = 5e-4\n",
    "lr_decay_type = \"cos\"\n",
    "\n",
    "save_period = 10\n",
    "save_dir = 'logs'\n",
    "eval_flag = True\n",
    "eval_period = 10\n",
    "num_workers = 4\n",
    "\n",
    "train_annotation_path = '2007_train.txt'\n",
    "val_annotation_path = '2007_val.txt'\n",
    "'''\n",
    "    with open('colab_config.py', 'w') as f:\n",
    "        f.write(resume_config)\n",
    "    print(\"‚úÖ Config updated for resume!\")\n",
    "else:\n",
    "    print(\"‚ùå No checkpoints found. Will start fresh training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a740c80a",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Start Training with Auto-Backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a00473d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "CHECKPOINT_DIR = '/content/drive/MyDrive/RDFNet_training_checkpoints'\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "os.makedirs('logs', exist_ok=True)\n",
    "\n",
    "def backup_checkpoints():\n",
    "    \"\"\"Backup checkpoints to Drive\"\"\"\n",
    "    for f in os.listdir('logs'):\n",
    "        if f.endswith('.pth'):\n",
    "            src = f'logs/{f}'\n",
    "            dst = f'{CHECKPOINT_DIR}/{f}'\n",
    "            if not os.path.exists(dst) or os.path.getmtime(src) > os.path.getmtime(dst):\n",
    "                shutil.copy2(src, dst)\n",
    "                print(f\"üíæ Backed up: {f}\")\n",
    "\n",
    "print(\"üöÄ Starting training...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    process = subprocess.Popen(\n",
    "        ['python', 'colab_train.py'],\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.STDOUT,\n",
    "        universal_newlines=True,\n",
    "        bufsize=1\n",
    "    )\n",
    "    \n",
    "    last_backup = time.time()\n",
    "    \n",
    "    for line in process.stdout:\n",
    "        print(line, end='')\n",
    "        \n",
    "        # Backup every 5 minutes\n",
    "        if time.time() - last_backup > 300:\n",
    "            backup_checkpoints()\n",
    "            last_backup = time.time()\n",
    "    \n",
    "    process.wait()\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n‚ö†Ô∏è Training interrupted!\")\n",
    "    process.terminate()\n",
    "\n",
    "finally:\n",
    "    backup_checkpoints()\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"‚úÖ Checkpoints saved to Drive!\")\n",
    "    print(f\"üìÅ Location: {CHECKPOINT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c983b7b",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Alternative: Simple Training (if Cell 6 has issues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe878f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this if Cell 6 has issues\n",
    "!python colab_train.py\n",
    "\n",
    "# Backup checkpoints\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "CHECKPOINT_DIR = '/content/drive/MyDrive/RDFNet_training_checkpoints'\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "for f in os.listdir('logs'):\n",
    "    if f.endswith('.pth'):\n",
    "        shutil.copy2(f'logs/{f}', f'{CHECKPOINT_DIR}/{f}')\n",
    "        print(f\"üíæ Backed up: {f}\")\n",
    "\n",
    "print(\"\\n‚úÖ All checkpoints backed up!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79ed885",
   "metadata": {},
   "source": [
    "---\n",
    "# üìä Evaluation on RTTS\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c9f294",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Find Best Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00f5284",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "CHECKPOINT_DIR = '/content/drive/MyDrive/RDFNet_training_checkpoints'\n",
    "os.makedirs('logs', exist_ok=True)\n",
    "\n",
    "# Find best checkpoint\n",
    "best_checkpoint = None\n",
    "\n",
    "for loc in ['logs', CHECKPOINT_DIR]:\n",
    "    if os.path.exists(loc):\n",
    "        files = [f for f in os.listdir(loc) if f.endswith('.pth')]\n",
    "        \n",
    "        # Prefer 'best'\n",
    "        best_files = [f for f in files if 'best' in f.lower()]\n",
    "        if best_files:\n",
    "            best_checkpoint = os.path.join(loc, best_files[0])\n",
    "            break\n",
    "        \n",
    "        # Otherwise latest epoch\n",
    "        epoch_files = [f for f in files if f.startswith('ep')]\n",
    "        if epoch_files:\n",
    "            epoch_files.sort(key=lambda x: int(x.split('-')[0][2:]), reverse=True)\n",
    "            best_checkpoint = os.path.join(loc, epoch_files[0])\n",
    "            break\n",
    "\n",
    "if best_checkpoint:\n",
    "    print(f\"üìå Using checkpoint: {best_checkpoint}\")\n",
    "    \n",
    "    # Copy to local if from Drive\n",
    "    if CHECKPOINT_DIR in best_checkpoint:\n",
    "        local_path = f\"logs/{os.path.basename(best_checkpoint)}\"\n",
    "        shutil.copy2(best_checkpoint, local_path)\n",
    "        best_checkpoint = local_path\n",
    "        print(f\"üìã Copied to: {local_path}\")\n",
    "else:\n",
    "    print(\"‚ùå No checkpoint found!\")\n",
    "    print(\"   Using pretrained: model_data/RDFNet.pth\")\n",
    "    best_checkpoint = 'model_data/RDFNet.pth'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842fe71e",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Evaluate on RTTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a916dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "from yolo import YOLO\n",
    "from utils.utils_map import get_map\n",
    "\n",
    "# Paths\n",
    "TEST_IMG = '/content/drive/MyDrive/dataset/RTTS/VOC2007/JPEGImages'\n",
    "TEST_ANN = '/content/drive/MyDrive/dataset/RTTS/VOC2007/Annotations'\n",
    "RTTS_CLASSES = ['bicycle', 'bus', 'car', 'motorbike', 'person']\n",
    "\n",
    "# Find checkpoint (from previous cell)\n",
    "model_path = best_checkpoint if 'best_checkpoint' in dir() else 'model_data/RDFNet.pth'\n",
    "print(f\"üì¶ Using: {model_path}\")\n",
    "\n",
    "# Initialize YOLO\n",
    "print(\"\\nüîÑ Loading model...\")\n",
    "yolo = YOLO(\n",
    "    model_path=model_path,\n",
    "    classes_path='model_data/rtts_classes.txt',\n",
    "    anchors_path='model_data/yolo_anchors.txt',\n",
    "    input_shape=[640, 640],\n",
    "    phi='l',\n",
    "    confidence=0.5,\n",
    "    nms_iou=0.3,\n",
    "    cuda=True\n",
    ")\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs('map_out/ground-truth', exist_ok=True)\n",
    "os.makedirs('map_out/detection-results', exist_ok=True)\n",
    "\n",
    "# Process images\n",
    "xml_files = [f for f in os.listdir(TEST_ANN) if f.endswith('.xml')]\n",
    "print(f\"\\nüìä Processing {len(xml_files)} images...\")\n",
    "\n",
    "for xml_file in tqdm(xml_files):\n",
    "    img_id = xml_file[:-4]\n",
    "    \n",
    "    # Find image\n",
    "    img_path = None\n",
    "    for ext in ['.jpg', '.png', '.jpeg', '.JPG']:\n",
    "        p = os.path.join(TEST_IMG, img_id + ext)\n",
    "        if os.path.exists(p):\n",
    "            img_path = p\n",
    "            break\n",
    "    \n",
    "    if not img_path:\n",
    "        continue\n",
    "    \n",
    "    # Ground truth\n",
    "    tree = ET.parse(os.path.join(TEST_ANN, xml_file))\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    with open(f'map_out/ground-truth/{img_id}.txt', 'w') as f:\n",
    "        for obj in root.iter('object'):\n",
    "            name = obj.find('name').text.lower()\n",
    "            if name not in RTTS_CLASSES:\n",
    "                continue\n",
    "            bbox = obj.find('bndbox')\n",
    "            xmin = int(float(bbox.find('xmin').text))\n",
    "            ymin = int(float(bbox.find('ymin').text))\n",
    "            xmax = int(float(bbox.find('xmax').text))\n",
    "            ymax = int(float(bbox.find('ymax').text))\n",
    "            f.write(f\"{name} {xmin} {ymin} {xmax} {ymax}\\n\")\n",
    "    \n",
    "    # Detection\n",
    "    try:\n",
    "        image = Image.open(img_path)\n",
    "        yolo.get_map_txt(img_id, image, RTTS_CLASSES, 'map_out')\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {img_id} - {e}\")\n",
    "\n",
    "# Calculate mAP\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä Calculating mAP...\")\n",
    "print(\"=\"*60)\n",
    "get_map(0.5, True, path='map_out')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ac60cf",
   "metadata": {},
   "source": [
    "## üîü Save Results to Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd9979a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "RESULTS_DIR = '/content/drive/MyDrive/RDFNet_results'\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "# Copy mAP results\n",
    "if os.path.exists('map_out'):\n",
    "    shutil.copytree('map_out', f'{RESULTS_DIR}/map_out', dirs_exist_ok=True)\n",
    "    print(\"‚úÖ Copied mAP results\")\n",
    "\n",
    "# Copy best checkpoint\n",
    "for loc in ['logs', '/content/drive/MyDrive/RDFNet_training_checkpoints']:\n",
    "    if os.path.exists(loc):\n",
    "        for f in os.listdir(loc):\n",
    "            if ('best' in f.lower() or 'last' in f.lower()) and f.endswith('.pth'):\n",
    "                shutil.copy2(os.path.join(loc, f), f'{RESULTS_DIR}/{f}')\n",
    "                print(f\"‚úÖ Copied: {f}\")\n",
    "\n",
    "print(f\"\\nüìÅ Results saved to: {RESULTS_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
